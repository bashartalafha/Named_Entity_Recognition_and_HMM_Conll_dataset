{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition on Conll(2003) dataset using pycrfcuite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook covers the following topics:\n",
    "1. Created a function to define features for training the model. We have used identity, word-suffix, word-shape, pos-tag and two previous and next adjacent neighboring words, as features. \n",
    "2. Trained the model using pycrfsuite. \n",
    "3. Performed hyperparameter tuning using l1, l2, and max_iterations features. \n",
    "4. Created a function to print classification report for each parameter setting. \n",
    "5. Found the best parameter setting on a validation set(eng.testa)\n",
    "6. Reporting the results on a testset (eng.testb)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conll 2003 dataset contains the following sub-datasets:\n",
    "  1. eng.train : Used as a  training set.\n",
    "  2. eng.testa : Used as a validation set.\n",
    "  3. eng.testb : Used as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we have downloaded the Conll 2003 dataset and are not using nltk.corpus for extracting the dataset directly, lets write the function to read datasets from the directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    count = 0\n",
    "    with open(filename) as f:\n",
    "        tmp = []\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            if count < 3:\n",
    "                continue                 \n",
    "            words = line.strip().split(\" \")\n",
    "            if len(words) < 2:\n",
    "                data.append(tmp)    \n",
    "                tmp = []\n",
    "            elif len(words) == 4:\n",
    "                tmp.append((words[0], words[1], words[3]))\n",
    "    #             print(tmp)\n",
    "            else:\n",
    "                print(count)\n",
    "                sys.exit(0)\n",
    "    return data\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng.train is used for training the model and eng.testa is used as a validation set. \n",
    "train_set = read_data(\"eng.train\")\n",
    "validation_set = read_data(\"eng.testa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CRICKET', 'NNP', 'O'),\n",
       " ('-', ':', 'O'),\n",
       " ('LEICESTERSHIRE', 'NNP', 'B-ORG'),\n",
       " ('TAKE', 'NNP', 'O'),\n",
       " ('OVER', 'IN', 'O'),\n",
       " ('AT', 'NNP', 'O'),\n",
       " ('TOP', 'NNP', 'O'),\n",
       " ('AFTER', 'NNP', 'O'),\n",
       " ('INNINGS', 'NNP', 'O'),\n",
       " ('VICTORY', 'NN', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' For our purpose, only word, its pos-tag, and its label is imp. Here ORG refers to \"organization\", LOC - \"location\",\n",
    "PER - \"Person\", B - \"Beggining\" E - End. '''\n",
    "validation_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the feature extractor. For a word, we extract its suffix (character n-grams), postag, case (lower or upper), and previous two and next two neighboring words as features. Special handling is done for boundary words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    postag = sentence[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'postag[:2]=' + postag[:2],\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sentence[i-1][0]\n",
    "        postag1 = sentence[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "        if i > 1:\n",
    "            word2 = sentence[i-2][0]\n",
    "            postag2 = sentence[i-2][1]\n",
    "            features.extend([\n",
    "                '-2:word.lower=' + word2.lower(),\n",
    "                '-2:word.istitle=%s' % word2.istitle(),\n",
    "                '-2.word.isupper=%s' % word2.isupper(),\n",
    "                '-2:postag=' + postag2,\n",
    "                '-2:postag[:2]=' + postag2[:2],\n",
    "            ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sentence)-1:\n",
    "        word1 = sentence[i+1][0]\n",
    "        postag1 = sentence[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "        if i < len(sentence)-2:\n",
    "            word2 = sentence[i+2][0]\n",
    "            postag2 = sentence[i+2][1]\n",
    "            features.extend([\n",
    "                '+2:word.lower=' + word2.lower(),\n",
    "                '+2:word.istitle=%s' % word2.istitle(),\n",
    "                '+2:word.isupper=%s' % word2.isupper(),\n",
    "                '+2:postag=' + postag2,\n",
    "                '+2:postag[:2]=' + postag2[:2],\n",
    "            ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "    return features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_features(sentence):\n",
    "    return [word2features(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "def extract_labels(sentence):\n",
    "    return [label for token, postag, label in sentence]\n",
    "\n",
    "def extract_tokens(sentence):\n",
    "    return [token for token, postag, label in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [define_features(s) for s in train_set]\n",
    "y_train = [extract_labels(s) for s in train_set]\n",
    "\n",
    "X_valid = [define_features(s) for s in validation_set]\n",
    "y_valid = [extract_labels(s) for s in validation_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one sample to see how X_train looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bias',\n",
       "  'word.lower=eu',\n",
       "  'word[-3:]=EU',\n",
       "  'word[-2:]=EU',\n",
       "  'word.isupper=True',\n",
       "  'word.istitle=False',\n",
       "  'word.isdigit=False',\n",
       "  'postag=NNP',\n",
       "  'postag[:2]=NN',\n",
       "  'BOS',\n",
       "  '+1:word.lower=rejects',\n",
       "  '+1:word.istitle=False',\n",
       "  '+1:word.isupper=False',\n",
       "  '+1:postag=VBZ',\n",
       "  '+1:postag[:2]=VB',\n",
       "  '+2:word.lower=german',\n",
       "  '+2:word.istitle=True',\n",
       "  '+2:word.isupper=False',\n",
       "  '+2:postag=JJ',\n",
       "  '+2:postag[:2]=JJ'],\n",
       " ['bias',\n",
       "  'word.lower=rejects',\n",
       "  'word[-3:]=cts',\n",
       "  'word[-2:]=ts',\n",
       "  'word.isupper=False',\n",
       "  'word.istitle=False',\n",
       "  'word.isdigit=False',\n",
       "  'postag=VBZ',\n",
       "  'postag[:2]=VB',\n",
       "  '-1:word.lower=eu',\n",
       "  '-1:word.istitle=False',\n",
       "  '-1:word.isupper=True',\n",
       "  '-1:postag=NNP',\n",
       "  '-1:postag[:2]=NN',\n",
       "  '+1:word.lower=german',\n",
       "  '+1:word.istitle=True',\n",
       "  '+1:word.isupper=False',\n",
       "  '+1:postag=JJ',\n",
       "  '+1:postag[:2]=JJ',\n",
       "  '+2:word.lower=call',\n",
       "  '+2:word.istitle=False',\n",
       "  '+2:word.isupper=False',\n",
       "  '+2:postag=NN',\n",
       "  '+2:postag[:2]=NN'],\n",
       " ['bias',\n",
       "  'word.lower=german',\n",
       "  'word[-3:]=man',\n",
       "  'word[-2:]=an',\n",
       "  'word.isupper=False',\n",
       "  'word.istitle=True',\n",
       "  'word.isdigit=False',\n",
       "  'postag=JJ',\n",
       "  'postag[:2]=JJ',\n",
       "  '-1:word.lower=rejects',\n",
       "  '-1:word.istitle=False',\n",
       "  '-1:word.isupper=False',\n",
       "  '-1:postag=VBZ',\n",
       "  '-1:postag[:2]=VB',\n",
       "  '-2:word.lower=eu',\n",
       "  '-2:word.istitle=False',\n",
       "  '-2.word.isupper=True',\n",
       "  '-2:postag=NNP',\n",
       "  '-2:postag[:2]=NN',\n",
       "  '+1:word.lower=call',\n",
       "  '+1:word.istitle=False',\n",
       "  '+1:word.isupper=False',\n",
       "  '+1:postag=NN',\n",
       "  '+1:postag[:2]=NN',\n",
       "  '+2:word.lower=to',\n",
       "  '+2:word.istitle=False',\n",
       "  '+2:word.isupper=False',\n",
       "  '+2:postag=TO',\n",
       "  '+2:postag[:2]=TO'],\n",
       " ['bias',\n",
       "  'word.lower=call',\n",
       "  'word[-3:]=all',\n",
       "  'word[-2:]=ll',\n",
       "  'word.isupper=False',\n",
       "  'word.istitle=False',\n",
       "  'word.isdigit=False',\n",
       "  'postag=NN',\n",
       "  'postag[:2]=NN',\n",
       "  '-1:word.lower=german',\n",
       "  '-1:word.istitle=True',\n",
       "  '-1:word.isupper=False',\n",
       "  '-1:postag=JJ',\n",
       "  '-1:postag[:2]=JJ',\n",
       "  '-2:word.lower=rejects',\n",
       "  '-2:word.istitle=False',\n",
       "  '-2.word.isupper=False',\n",
       "  '-2:postag=VBZ',\n",
       "  '-2:postag[:2]=VB',\n",
       "  '+1:word.lower=to',\n",
       "  '+1:word.istitle=False',\n",
       "  '+1:word.isupper=False',\n",
       "  '+1:postag=TO',\n",
       "  '+1:postag[:2]=TO',\n",
       "  '+2:word.lower=boycott',\n",
       "  '+2:word.istitle=False',\n",
       "  '+2:word.isupper=False',\n",
       "  '+2:postag=VB',\n",
       "  '+2:postag[:2]=VB'],\n",
       " ['bias',\n",
       "  'word.lower=to',\n",
       "  'word[-3:]=to',\n",
       "  'word[-2:]=to',\n",
       "  'word.isupper=False',\n",
       "  'word.istitle=False',\n",
       "  'word.isdigit=False',\n",
       "  'postag=TO',\n",
       "  'postag[:2]=TO',\n",
       "  '-1:word.lower=call',\n",
       "  '-1:word.istitle=False',\n",
       "  '-1:word.isupper=False',\n",
       "  '-1:postag=NN',\n",
       "  '-1:postag[:2]=NN',\n",
       "  '-2:word.lower=german',\n",
       "  '-2:word.istitle=True',\n",
       "  '-2.word.isupper=False',\n",
       "  '-2:postag=JJ',\n",
       "  '-2:postag[:2]=JJ',\n",
       "  '+1:word.lower=boycott',\n",
       "  '+1:word.istitle=False',\n",
       "  '+1:word.isupper=False',\n",
       "  '+1:postag=VB',\n",
       "  '+1:postag[:2]=VB',\n",
       "  '+2:word.lower=british',\n",
       "  '+2:word.istitle=True',\n",
       "  '+2:word.isupper=False',\n",
       "  '+2:postag=JJ',\n",
       "  '+2:postag[:2]=JJ'],\n",
       " ['bias',\n",
       "  'word.lower=boycott',\n",
       "  'word[-3:]=ott',\n",
       "  'word[-2:]=tt',\n",
       "  'word.isupper=False',\n",
       "  'word.istitle=False',\n",
       "  'word.isdigit=False',\n",
       "  'postag=VB',\n",
       "  'postag[:2]=VB',\n",
       "  '-1:word.lower=to',\n",
       "  '-1:word.istitle=False',\n",
       "  '-1:word.isupper=False',\n",
       "  '-1:postag=TO',\n",
       "  '-1:postag[:2]=TO',\n",
       "  '-2:word.lower=call',\n",
       "  '-2:word.istitle=False',\n",
       "  '-2.word.isupper=False',\n",
       "  '-2:postag=NN',\n",
       "  '-2:postag[:2]=NN',\n",
       "  '+1:word.lower=british',\n",
       "  '+1:word.istitle=True',\n",
       "  '+1:word.isupper=False',\n",
       "  '+1:postag=JJ',\n",
       "  '+1:postag[:2]=JJ',\n",
       "  '+2:word.lower=lamb',\n",
       "  '+2:word.istitle=False',\n",
       "  '+2:word.isupper=False',\n",
       "  '+2:postag=NN',\n",
       "  '+2:postag[:2]=NN'],\n",
       " ['bias',\n",
       "  'word.lower=british',\n",
       "  'word[-3:]=ish',\n",
       "  'word[-2:]=sh',\n",
       "  'word.isupper=False',\n",
       "  'word.istitle=True',\n",
       "  'word.isdigit=False',\n",
       "  'postag=JJ',\n",
       "  'postag[:2]=JJ',\n",
       "  '-1:word.lower=boycott',\n",
       "  '-1:word.istitle=False',\n",
       "  '-1:word.isupper=False',\n",
       "  '-1:postag=VB',\n",
       "  '-1:postag[:2]=VB',\n",
       "  '-2:word.lower=to',\n",
       "  '-2:word.istitle=False',\n",
       "  '-2.word.isupper=False',\n",
       "  '-2:postag=TO',\n",
       "  '-2:postag[:2]=TO',\n",
       "  '+1:word.lower=lamb',\n",
       "  '+1:word.istitle=False',\n",
       "  '+1:word.isupper=False',\n",
       "  '+1:postag=NN',\n",
       "  '+1:postag[:2]=NN',\n",
       "  '+2:word.lower=.',\n",
       "  '+2:word.istitle=False',\n",
       "  '+2:word.isupper=False',\n",
       "  '+2:postag=.',\n",
       "  '+2:postag[:2]=.'],\n",
       " ['bias',\n",
       "  'word.lower=lamb',\n",
       "  'word[-3:]=amb',\n",
       "  'word[-2:]=mb',\n",
       "  'word.isupper=False',\n",
       "  'word.istitle=False',\n",
       "  'word.isdigit=False',\n",
       "  'postag=NN',\n",
       "  'postag[:2]=NN',\n",
       "  '-1:word.lower=british',\n",
       "  '-1:word.istitle=True',\n",
       "  '-1:word.isupper=False',\n",
       "  '-1:postag=JJ',\n",
       "  '-1:postag[:2]=JJ',\n",
       "  '-2:word.lower=boycott',\n",
       "  '-2:word.istitle=False',\n",
       "  '-2.word.isupper=False',\n",
       "  '-2:postag=VB',\n",
       "  '-2:postag[:2]=VB',\n",
       "  '+1:word.lower=.',\n",
       "  '+1:word.istitle=False',\n",
       "  '+1:word.isupper=False',\n",
       "  '+1:postag=.',\n",
       "  '+1:postag[:2]=.'],\n",
       " ['bias',\n",
       "  'word.lower=.',\n",
       "  'word[-3:]=.',\n",
       "  'word[-2:]=.',\n",
       "  'word.isupper=False',\n",
       "  'word.istitle=False',\n",
       "  'word.isdigit=False',\n",
       "  'postag=.',\n",
       "  'postag[:2]=.',\n",
       "  '-1:word.lower=lamb',\n",
       "  '-1:word.istitle=False',\n",
       "  '-1:word.isupper=False',\n",
       "  '-1:postag=NN',\n",
       "  '-1:postag[:2]=NN',\n",
       "  '-2:word.lower=british',\n",
       "  '-2:word.istitle=True',\n",
       "  '-2.word.isupper=False',\n",
       "  '-2:postag=JJ',\n",
       "  '-2:postag[:2]=JJ',\n",
       "  'EOS']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below shown code is used to train the model using pycrfsuite. First, pycrfsuite.Trainer is defined and then dataset is loaded in crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for x_sent, y_sent in zip(X_train, y_train):\n",
    "    trainer.append(x_sent, y_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function generates the classification report and collects the F-1 score metric for each classified document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def F1_classification_report(y_true, y_pred):\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "\n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "\n",
    "    return f1_score(y_true_combined, y_pred_combined, labels = [class_indices[cls] for cls in tagset], average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below given code performs the parameter tuning and prints the F1 scores corresponding to each parameter setting, applied on validation set in sorted order. The parameter setting with maximum F1 score will be considered as the best setting acheived on validation set(eng.testa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting: l1 =  0.5 l2 = 0.001 max_iter =  50\n",
      "Setting: l1 =  0.5 l2 = 0.001 max_iter =  100\n",
      "Setting: l1 =  0.5 l2 = 0.001 max_iter =  150\n",
      "Setting: l1 =  0.5 l2 = 0.005 max_iter =  50\n",
      "Setting: l1 =  0.5 l2 = 0.005 max_iter =  100\n",
      "Setting: l1 =  0.5 l2 = 0.005 max_iter =  150\n",
      "Setting: l1 =  0.5 l2 = 0.01 max_iter =  50\n",
      "Setting: l1 =  0.5 l2 = 0.01 max_iter =  100\n",
      "Setting: l1 =  0.5 l2 = 0.01 max_iter =  150\n",
      "Setting: l1 =  1 l2 = 0.001 max_iter =  50\n",
      "Setting: l1 =  1 l2 = 0.001 max_iter =  100\n",
      "Setting: l1 =  1 l2 = 0.001 max_iter =  150\n",
      "Setting: l1 =  1 l2 = 0.005 max_iter =  50\n",
      "Setting: l1 =  1 l2 = 0.005 max_iter =  100\n",
      "Setting: l1 =  1 l2 = 0.005 max_iter =  150\n",
      "Setting: l1 =  1 l2 = 0.01 max_iter =  50\n",
      "Setting: l1 =  1 l2 = 0.01 max_iter =  100\n",
      "Setting: l1 =  1 l2 = 0.01 max_iter =  150\n",
      "Setting: l1 =  1.5 l2 = 0.001 max_iter =  50\n",
      "Setting: l1 =  1.5 l2 = 0.001 max_iter =  100\n",
      "Setting: l1 =  1.5 l2 = 0.001 max_iter =  150\n",
      "Setting: l1 =  1.5 l2 = 0.005 max_iter =  50\n",
      "Setting: l1 =  1.5 l2 = 0.005 max_iter =  100\n",
      "Setting: l1 =  1.5 l2 = 0.005 max_iter =  150\n",
      "Setting: l1 =  1.5 l2 = 0.01 max_iter =  50\n",
      "Setting: l1 =  1.5 l2 = 0.01 max_iter =  100\n",
      "Setting: l1 =  1.5 l2 = 0.01 max_iter =  150\n",
      "[('1.5_0.01_100', 0.8617941392384939), ('1.5_0.001_100', 0.8621120204873133), ('1.5_0.001_150', 0.8621443632311254), ('1.5_0.005_150', 0.8626568971248902), ('1.5_0.01_150', 0.8630252871967783), ('1.5_0.005_50', 0.8636030522857377), ('1.5_0.001_50', 0.8637187579876784), ('1.5_0.005_100', 0.8638472368960427), ('1.5_0.01_50', 0.8640451725970021), ('1_0.01_150', 0.8720598713407528), ('1_0.01_100', 0.872268082240182), ('1_0.005_100', 0.8723305060763998), ('1_0.001_50', 0.8723967913217965), ('1_0.005_150', 0.8727311784532172), ('1_0.001_150', 0.873065231782216), ('1_0.001_100', 0.8733643414553223), ('1_0.01_50', 0.8755007632874355), ('1_0.005_50', 0.8756863610865465), ('0.5_0.001_100', 0.8797404478050613), ('0.5_0.01_150', 0.8798433371726778), ('0.5_0.001_150', 0.8799533176007326), ('0.5_0.005_100', 0.8813927566267412), ('0.5_0.01_100', 0.881616728860429), ('0.5_0.005_150', 0.8818558681100269), ('0.5_0.01_50', 0.882625911984139), ('0.5_0.001_50', 0.884005478652661), ('0.5_0.005_50', 0.8842322200559327)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import operator\n",
    "F1_scores = {}\n",
    "\n",
    "l1_penalty = [0.5, 1, 1.5]\n",
    "l2_penalty = [0.001, 0.005, 0.01]\n",
    "max_iterations = [50, 100, 150]\n",
    "for (l1,l2, max_iter) in list(itertools.product(l1_penalty, l2_penalty, max_iterations)): \n",
    "    print(\"Setting: l1 = \",l1,\"l2 =\", l2, \"max_iter = \", max_iter)\n",
    "    trainer.set_params({\n",
    "        'c1': l1,   # coefficient for L1 penalty\n",
    "        'c2': l2,  # coefficient for L2 penalty\n",
    "        'max_iterations': max_iter,  # stop earlier\n",
    "        'feature.possible_transitions': True # include transitions that are possible, but not observed\n",
    "        })\n",
    "    trainer.train('crf-conll2003.model')\n",
    "    trainer.logparser.last_iteration\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open('crf-conll2003.model')\n",
    "\n",
    "\n",
    "    y_pred = [tagger.tag(xseq) for xseq in X_valid]\n",
    "    key = \"_\".join([str(l1), str(l2), str(max_iter)])\n",
    "    F1_scores[key] = F1_classification_report(y_valid, y_pred)\n",
    "sorted_F1 = sorted(F1_scores.items(), key=operator.itemgetter(1))\n",
    "print(sorted_F1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below given code reports the F1 score on the best parameter setting obtained on the validation set above. We want to see the difference in accuracies on validation set and the test set in order to evaluate the parameter settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two test sets, testa is used as validation set and testb is used as test set. \n",
    "test_set = read_data(\"eng.testb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SOCCER', 'NN', 'O'),\n",
       " ('-', ':', 'O'),\n",
       " ('JAPAN', 'NNP', 'B-LOC'),\n",
       " ('GET', 'VB', 'O'),\n",
       " ('LUCKY', 'NNP', 'O'),\n",
       " ('WIN', 'NNP', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('CHINA', 'NNP', 'B-PER'),\n",
       " ('IN', 'IN', 'O'),\n",
       " ('SURPRISE', 'DT', 'O'),\n",
       " ('DEFEAT', 'NN', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "# Using the same procedure as above, we will extract features for every sentence of this test set and seperates the label from it. \n",
    "X_test = [define_features(s) for s in test_set]\n",
    "y_test = [extract_labels(s) for s in test_set]\n",
    "\n",
    "for x_sent, y_sent in zip(X_train, y_train):   \n",
    "    trainer.append(x_sent, y_sent)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### During hyperparameter tuning on validation set, we found our best setting as: l1= 0.5, l2= 0.05, max_iterations = 50. Thus, we will apply this setting to our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f91f60d8048>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_penalty,l2_penalty,max_iterations  = 1.5, 0.005, 150\n",
    "trainer.set_params({\n",
    "    'c1': l1_penalty,   # coefficient for L1 penalty\n",
    "    'c2': l2_penalty,  # coefficient for L2 penalty\n",
    "    'max_iterations': max_iterations,  # stop earlier\n",
    "    'feature.possible_transitions': True # include transitions that are possible, but not observed\n",
    "    })\n",
    "trainer.train('crf-conll2003.model')\n",
    "trainer.logparser.last_iteration\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('crf-conll2003.model')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model will be trained on the same set (eng.train), which is used for training above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1.5_0.005_150', 0.7899300685849052)]\n"
     ]
    }
   ],
   "source": [
    "F1_scores_test_set = {}\n",
    "y_predb = [tagger.tag(x_sent) for x_sent in X_test]\n",
    "key = \"_\".join([str(l1_penalty), str(l2_penalty), str(max_iterations)])\n",
    "F1_scores_test_set[key] = F1_classification_report(y_test, y_predb)\n",
    "\n",
    "sorted_F1 = sorted(F1_scores_test_set.items(), key=operator.itemgetter(1))\n",
    "print(sorted_F1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Thus we find that, parameter tuning on validation set gave us the accuracy of 88.423 % and reporting the best setting obtained from validation set on test set gave us the accuracy of 80.504%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus we see that, there is an improvement of 20% approx, ( HMM - 61.81% to CRF- 78.78% ) in the classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
